import json
import re
import os
import ast
import argparse
from sklearn.metrics import mean_absolute_error, balanced_accuracy_score
from green_score import GREEN

# ================================================================================
# ARGUMENT PARSING
# ================================================================================

def parse_args():
    """Parse command line arguments for evaluation script."""
    parser = argparse.ArgumentParser(
        description="PaliGemma2 evaluation script for medical image analysis",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # Data Configuration
    data_group = parser.add_argument_group('Data Configuration')
    data_group.add_argument(
        "--base_dataset_path", 
        type=str, 
        default="original_dataset",
        help="Base path to dataset directory containing validation data"
    )
    data_group.add_argument(
        "--prediction_file", 
        type=str, 
        required=True,
        default="predictions.json",
        help="Path to prediction JSON file generated by the model"
    )

    # Output Configuration
    output_group = parser.add_argument_group('Output Configuration')
    output_group.add_argument(
        "--output_dir",
        type=str,
        default=".",
        help="Output directory for evaluation metrics"
    )
    output_group.add_argument(
        "--output_filename",
        type=str,
        default="metrics.json",
        help="Filename for the output metrics file"
    )
    
    return parser.parse_args()


# ================================================================================
# UTILITY FUNCTIONS
# ================================================================================

def find_json_files(base_path):
    """Recursively find all JSON files in the specified directory."""
    json_files = []
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file.endswith('.json'):
                json_files.append(os.path.join(root, file))
    return json_files


def load_and_merge_json_files(json_files):
    """Load and merge multiple JSON files into a single list."""
    all_data = []
    for json_file in json_files:
        try:
            with open(json_file, 'r') as f:
                data = json.load(f)
                if isinstance(data, list):
                    all_data.extend(data)
                else:
                    all_data.append(data)
            print(f"Loaded {len(data) if isinstance(data, list) else 1} samples from {os.path.basename(json_file)}")
        except Exception as e:
            print(f"Warning: Failed to load {json_file}: {e}")
    return all_data


def create_sample_key(sample):
    """Create a unique key for matching samples between ground truth and predictions."""
    image_name = str(sample.get("ImageName", sample.get("image", "")))
    question = str(sample.get("Question", ""))
    return f"{image_name}||{question}"


def validate_paths(ground_truth_path, prediction_file):
    """Validate that required paths exist."""
    # Check ground truth path
    if not os.path.exists(ground_truth_path):
        raise FileNotFoundError(f"Ground truth dataset directory not found: {ground_truth_path}")
    
    # Find JSON files in ground truth path
    gt_files = find_json_files(ground_truth_path)
    if not gt_files:
        raise FileNotFoundError(f"No JSON files found in {ground_truth_path}")
    
    # Check prediction file
    if not os.path.exists(prediction_file):
        raise FileNotFoundError(f"Prediction file not found: {prediction_file}")
    
    return gt_files, True


# ================================================================================
# METRIC CALCULATION FUNCTIONS
# ================================================================================

def calculate_iou(bbox1, bbox2):
    """Calculate Intersection over Union (IoU) between two bounding boxes."""
    try:
        x1_min, y1_min, x1_max, y1_max = bbox1
        x2_min, y2_min, x2_max, y2_max = bbox2
        
        # Calculate intersection coordinates
        x_left = max(x1_min, x2_min)
        y_top = max(y1_min, y2_min)
        x_right = min(x1_max, x2_max)
        y_bottom = min(y1_max, y2_max)
        
        # Check if there's no intersection
        if x_right <= x_left or y_bottom <= y_top:
            return 0.0
        
        # Calculate areas
        intersection_area = (x_right - x_left) * (y_bottom - y_top)
        bbox1_area = (x1_max - x1_min) * (y1_max - y1_min)
        bbox2_area = (x2_max - x2_min) * (y2_max - y2_min)
        union_area = bbox1_area + bbox2_area - intersection_area
        
        return intersection_area / union_area if union_area > 0 else 0.0
    except Exception:
        return 0.0


def calculate_classification_metrics(predictions, ground_truth):
    """Calculate metrics for classification tasks."""
    normalized_gt = [
        str(ref).strip().lower() if ref is not None else ""
        for ref in ground_truth
    ]
    normalized_pred = [
        str(pred).strip().lower() if pred is not None else ""
        for pred in predictions
    ]
    return {
        "balanced_accuracy": balanced_accuracy_score(normalized_gt, normalized_pred)
    }


def calculate_multilabel_metrics(predictions, ground_truth):
    """Calculate metrics for multi-label classification tasks."""
    true_positives = false_positives = false_negatives = 0
    
    for pred, gt in zip(predictions, ground_truth):
        if isinstance(pred, str) and isinstance(gt, str):
            pred_labels = set(label.strip().lower() for label in re.split(r'[;,]', pred) if label.strip())
            gt_labels = set(label.strip().lower() for label in re.split(r'[;,]', gt) if label.strip())
            
            true_positives += len(pred_labels & gt_labels)
            false_positives += len(pred_labels - gt_labels)
            false_negatives += len(gt_labels - pred_labels)
    
    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0
    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0
    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    
    return {"f1_score": f1_score}


def calculate_detection_metrics(predictions, ground_truth):
    """Calculate metrics for detection tasks."""
    true_positives = false_positives = false_negatives = 0
    
    for pred, gt in zip(predictions, ground_truth):
        try:
            if isinstance(gt, str): 
                gt = ast.literal_eval(gt)
            if isinstance(pred, str): 
                pred = ast.literal_eval(pred)
            
            if not isinstance(pred, list):
                false_negatives += len(gt)
                continue
            
            matched_predictions = set()
            
            for gt_bbox in gt:
                best_iou, best_idx = 0.0, -1
                
                for idx, pred_bbox in enumerate(pred):
                    if idx in matched_predictions: 
                        continue
                    iou = calculate_iou(gt_bbox, pred_bbox)
                    if iou > best_iou: 
                        best_iou, best_idx = iou, idx
                
                if best_iou > 0.5:
                    true_positives += 1
                    matched_predictions.add(best_idx)
                else:
                    false_negatives += 1
            
            false_positives += len(pred) - len(matched_predictions)
            
        except Exception:
            continue
    
    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0
    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0
    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    
    return {"detection_f1": f1_score}


def calculate_regression_metrics(predictions, ground_truth):
    """Calculate metrics for regression and counting tasks."""
    def safe_float_conversion(value):
        try:
            return float(value)
        except Exception:
            return None 
    
    gt_floats = [safe_float_conversion(x) for x in ground_truth]
    pred_floats = [safe_float_conversion(x) for x in predictions]
    
    if None in gt_floats or None in pred_floats:
        return {"mean_absolute_error": float('inf')}

    valid_pairs = list(zip(pred_floats, gt_floats))
    if not valid_pairs:
        return {"mean_absolute_error": None}
    
    preds, gts = zip(*valid_pairs)
    return {"mean_absolute_error": mean_absolute_error(gts, preds)}


def calculate_instance_detection_metrics(predictions, ground_truth):
    """Calculate metrics for instance detection tasks."""
    true_positives = false_positives = false_negatives = 0
    
    for pred, gt in zip(predictions, ground_truth):
        try:
            if isinstance(gt, str): 
                gt = json.loads(gt)
            if isinstance(pred, str): 
                pred = json.loads(pred)
            
            if not isinstance(pred, dict):
                total_fn = sum(len(v) for v in gt.values() if isinstance(v, list))
                false_negatives += total_fn
                continue
            
            all_classes = set(gt.keys()) | set(pred.keys())
            
            for class_name in all_classes:
                gt_bboxes = gt.get(class_name, [])
                pred_bboxes = pred.get(class_name, [])
                
                gt_matched = set()
                pred_matched = set()
                
                # Create IoU matrix
                iou_matrix = [[calculate_iou(gt_box, pred_box) 
                              for pred_box in pred_bboxes] 
                              for gt_box in gt_bboxes]
                
                while True:
                    max_iou, max_gt_idx, max_pred_idx = -1, -1, -1
                    
                    for i, row in enumerate(iou_matrix):
                        if i in gt_matched: 
                            continue
                        for j, iou_value in enumerate(row):
                            if j in pred_matched: 
                                continue
                            if iou_value > max_iou:
                                max_iou, max_gt_idx, max_pred_idx = iou_value, i, j
                    
                    if max_iou >= 0.5:
                        true_positives += 1
                        gt_matched.add(max_gt_idx)
                        pred_matched.add(max_pred_idx)
                    else:
                        break
                
                false_negatives += len(gt_bboxes) - len(gt_matched)
                false_positives += len(pred_bboxes) - len(pred_matched)
                
        except Exception:
            continue
    
    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0
    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0
    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    
    return {"instance_f1": f1_score}

def calculate_report_generation_metrics(predictions, ground_truth):
    """Calculate metrics for report generation tasks."""
    green_model_id = "StanfordAIMI/GREEN-radllama2-7b"
    green_scorer = GREEN(green_model_id, output_dir=".")
    try:
        mean, _, _, _, _ = green_scorer(ground_truth, predictions)
        return {"green_score": mean}
    except:
        return {"green_score": None}

def calculate_task_metrics(predictions, ground_truth, task_type):
    """Calculate appropriate metrics based on task type."""
    task_type_normalized = task_type.lower().strip()
    
    if task_type_normalized == "classification":
        return calculate_classification_metrics(predictions, ground_truth)
    elif task_type_normalized == "multi-label classification":
        return calculate_multilabel_metrics(predictions, ground_truth)
    elif task_type_normalized == "detection":
        return calculate_detection_metrics(predictions, ground_truth)
    elif task_type_normalized in ("regression", "counting"):
        return calculate_regression_metrics(predictions, ground_truth)
    elif task_type_normalized == "instance_detection":
        return calculate_instance_detection_metrics(predictions, ground_truth)
    elif task_type_normalized in ("report_generation"):
        return calculate_report_generation_metrics(predictions, ground_truth)
    else:
        return {}


# ================================================================================
# MAIN PROCESSING FUNCTION
# ================================================================================

def run_evaluation(args):
    """Main function to run evaluation comparing predictions with ground truth."""
    # Construct ground truth dataset path
    gt_dataset_path = os.path.join(args.base_dataset_path, "validation-public")
    
    # Validate paths
    print("Validating paths and discovering files...")
    gt_files, _ = validate_paths(gt_dataset_path, args.prediction_file)
    
    # Load ground truth data from validation-public directory
    print(f"Loading ground truth data from {gt_dataset_path}...")
    ground_truth_data = load_and_merge_json_files(gt_files)
    print(f"Total ground truth samples: {len(ground_truth_data)}")
    
    # Load prediction data
    print(f"Loading prediction data from {args.prediction_file}...")
    with open(args.prediction_file, 'r') as f:
        prediction_data = json.load(f)
    print(f"Total prediction samples: {len(prediction_data)}")
    
    # Create lookup dictionaries
    gt_lookup = {create_sample_key(sample): sample for sample in ground_truth_data}
    pred_lookup = {create_sample_key(sample): sample for sample in prediction_data}
    
    # Debug: Print some sample matching info
    print(f"\nSample matching info:")
    print(f"GT lookup size: {len(gt_lookup)}")
    print(f"Pred lookup size: {len(pred_lookup)}")
    assert len(gt_lookup) == len(pred_lookup)
    
    # Group data by task type
    task_type_to_gt = {}
    task_type_to_pred = {}
    
    matched_samples = 0
    unmatched_samples = 0
    
    for sample_key, gt_sample in gt_lookup.items():
        task_type = gt_sample.get("TaskType", "").strip().lower()
        
        if not task_type:
            continue
            
        pred_sample = pred_lookup.get(sample_key)
        if pred_sample is None:
            unmatched_samples += 1
            continue
        
        gt_answer = gt_sample.get("Answer", "")
        pred_answer = pred_sample.get("Answer", "")
        
        task_type_to_gt.setdefault(task_type, []).append(gt_answer)
        task_type_to_pred.setdefault(task_type, []).append(pred_answer)
        matched_samples += 1
    
    print(f"Successfully matched {matched_samples} samples for evaluation")
    print(f"Unmatched samples: {unmatched_samples}")
    
    # Calculate metrics for each task type
    print(f"\nCalculating metrics by task type...")
    evaluation_results = {}
    total_evaluated = 0
    
    for task_type in sorted(task_type_to_gt.keys()):
        gt_answers = task_type_to_gt[task_type]
        pred_answers = task_type_to_pred[task_type]
        
        metrics = calculate_task_metrics(pred_answers, gt_answers, task_type)
        metrics["num_examples"] = len(gt_answers)
        evaluation_results[task_type] = metrics
        total_evaluated += len(gt_answers)
        
        print(f"\n{task_type}:")
        print(f"  samples: {len(gt_answers)}")
        for metric_name, metric_value in metrics.items():
            if metric_name != "num_examples":
                if isinstance(metric_value, float):
                    print(f"  {metric_name}: {metric_value:.4f}")
                else:
                    print(f"  {metric_name}: {metric_value}")
    
    print(f"\nTotal evaluated samples: {total_evaluated}")
    
    # Save results
    print(f"\nSaving evaluation results...")
    os.makedirs(args.output_dir, exist_ok=True)
    output_file = os.path.join(args.output_dir, args.output_filename)
    
    with open(output_file, "w") as f:
        json.dump(evaluation_results, f, indent=2)
    
    print(f"Evaluation metrics saved to: {output_file}")
    
    return evaluation_results


# ================================================================================
# SCRIPT ENTRY POINT
# ================================================================================

if __name__ == "__main__":
    args = parse_args()
    
    print("PaliGemma2 Medical Image Evaluation")
    print("=" * 50)
    print(f"Base dataset path: {args.base_dataset_path}")
    print(f"Full GT path: {os.path.join(args.base_dataset_path, 'validation-public')}")
    print(f"Prediction file: {args.prediction_file}")
    print(f"Output directory: {args.output_dir}")
    
    try:
        results = run_evaluation(args)
        total_tasks = len(results)
        print(f"\nSuccessfully evaluated {total_tasks} task types")
    except Exception as e:
        print(f"\nError: {e}")
        raise 